import numpy as np
import math
from sklearn.naive_bayes import GaussianNB

# 连续型数据分类用正态分布公式
def getPro(theData, mean, var):
    pro = 1 / (math.sqrt(2 * math.pi) * math.sqrt(var)) * math.exp(-(theData - mean) ** 2 / (2 * var))
    return pro


def getRandom(num):
    Ran = np.random.dirichlet(np.ones(num), size = 1)
    Ran = Ran.flatten()
    return Ran


'''
def CountP1(test):
    sum=1
    for i in range(0,60):
       sum*=getPro(test[i],
def CountP2(test):
    sum=1
    for i in  range(0,60):
        sum*=getPro(())
'''

X = np.loadtxt('[018]musk01(0-1).txt')
# 其中有97
m = X.shape[1] - 1  # 属性数量
n = X.shape[0]  # 样本数目
T = 6
K = 2  # 类标记数量
# 主要过程：分组
# 去掉类标记
Class1 = 0
Class2 = 0

for i in X:
    if i[m] == 1:
        Class1 = Class1 + 1
    elif i[m] == 2:
        Class2 = Class2 + 1

train1 = int(Class1 * 0.5)
val1 = int(Class1 * 0.2)
test1 = Class1 - train1 - val1

train2 = int(Class2 * 0.5)
val2 = int(Class2 * 0.2)
test2 = Class2 - train2 - val2

# 随机产生多少个和为1的随机数W
G1 = [16, 23, 29, 31, 40, 61, 63, 89, 98, 100, 116, 117, 124, 155, 158] # 7
G2 = [1, 6, 21, 24, 33, 38, 52, 56, 64, 68, 70, 76, 83, 84, 85, 99, 113, 118, 119, 120, 122, 127, 131, 143, 153]# 3
G3 = [13, 15, 41, 45, 53, 58, 69, 102, 135, 147]
G4 = [2, 3, 5, 8, 22, 25, 26, 44, 51, 54, 57, 59, 60, 71, 81, 91, 92, 95, 96, 97, 112, 114, 121, 123, 129, 136, 138, 142, 149, 151, 154, 160]
G5 = [0, 4, 9, 10, 11, 14, 17, 18, 19, 20, 27, 34, 35, 36, 37, 39, 42, 43, 46, 47, 48, 49, 50, 55, 62, 65, 72, 73, 74, 75, 77, 78, 80, 82, 86, 87, 88, 90, 93, 94, 101, 103, 104, 105, 107, 108, 110, 111, 115, 125, 126, 128, 130, 132, 133, 137, 139, 140, 141, 144, 148, 152, 157, 159, 161, 163]
G6 = [7, 12, 28, 30, 32, 66, 67, 79, 106, 109, 134, 145, 146, 150, 156, 162, 164, 165]  # 12
len1=len(G1)
len2=len(G2)
len3=len(G3)
len4=len(G4)
len5=len(G5)
len6=len(G6)
l1=len1
l2=len1+len2
l3=len1+len2+len3
l4=len1+len2+len3+len4
l5=len1+len2+len3+len4+len5
l6=len1+len2+len3+len4+len5+len6
#随机训练集，验证集，测试集区

idx = np.random.choice(np.arange(Class1), size = train1, replace = False)
train_index1 = np.array(idx)
val_index1 = np.random.choice(np.delete(np.arange(Class1), train_index1), size = val1, replace = False)
test_index1 = np.delete(np.arange(Class1), np.append(train_index1, val_index1))

idx1 = np.random.choice(np.arange(Class2), size = train2, replace = False)
train_index2 = np.array(idx1)
val_index2 = np.random.choice(np.delete(np.arange(Class2), train_index2), size = val2, replace = False)
test_index2 = np.delete(np.arange(Class2), np.append(train_index2, val_index2))

print("train_index1 =",list(train_index1))
print("val_index1 =",list(val_index1))
print("test_index1 =",list(test_index1))
print("train_index2 =",list(train_index2))
print("val_index2 =",list(val_index2))
print("test_index2 =",list(test_index2))




#确认训练集，验证集，测试集区
train_index1 = [160, 19, 112, 78, 66, 46, 180, 158, 16, 128, 164, 184, 99, 23, 157, 116, 83, 115, 175, 151, 3, 206, 79, 120, 72, 132, 144, 10, 162, 52, 111, 82, 45, 48, 30, 69, 60, 21, 80, 44, 204, 43, 76, 192, 167, 11, 113, 186, 6, 200, 94, 177, 0, 182, 203, 75, 171, 89, 196, 173, 194, 14, 198, 107, 202, 124, 24, 61, 197, 188, 91, 42, 77, 106, 7, 85, 170, 147, 159, 152, 174, 185, 31, 13, 68, 96, 114, 121, 138, 25, 166, 178, 5, 102, 59, 201, 26, 101, 119, 49, 187, 17, 191]
val_index1 = [41, 108, 143, 9, 136, 39, 81, 154, 37, 149, 51, 176, 50, 134, 155, 1, 35, 137, 98, 193, 36, 34, 126, 63, 127, 105, 123, 169, 54, 179, 142, 133, 62, 131, 2, 33, 28, 97, 181, 110, 29]
test_index1 = [4, 8, 12, 15, 18, 20, 22, 27, 32, 38, 40, 47, 53, 55, 56, 57, 58, 64, 65, 67, 70, 71, 73, 74, 84, 86, 87, 88, 90, 92, 93, 95, 100, 103, 104, 109, 117, 118, 122, 125, 129, 130, 135, 139, 140, 141, 145, 146, 148, 150, 153, 156, 161, 163, 165, 168, 172, 183, 189, 190, 195, 199, 205]
train_index2 = [138, 88, 99, 30, 108, 256, 77, 155, 257, 135, 93, 87, 245, 162, 171, 27, 260, 96, 146, 126, 150, 251, 105, 15, 229, 127, 81, 225, 258, 65, 18, 137, 190, 51, 9, 205, 113, 210, 166, 202, 35, 49, 22, 139, 72, 174, 145, 228, 45, 98, 97, 243, 160, 47, 112, 70, 236, 117, 84, 56, 76, 25, 67, 259, 250, 263, 101, 114, 54, 185, 249, 59, 209, 168, 233, 159, 31, 238, 44, 39, 23, 226, 62, 170, 8, 133, 2, 89, 82, 52, 177, 79, 80, 230, 254, 106, 46, 219, 182, 136, 240, 119, 156, 21, 196, 115, 107, 220, 262, 267, 28, 178, 242, 86, 75, 134, 264, 33, 143, 32, 206, 3, 237, 172, 13, 74, 69, 130, 235, 12, 124, 100, 152, 48]
val_index2 = [157, 41, 26, 261, 116, 193, 125, 6, 43, 218, 36, 34, 118, 188, 5, 203, 181, 241, 214, 50, 149, 231, 147, 239, 163, 248, 200, 92, 17, 223, 194, 73, 253, 38, 175, 90, 186, 207, 40, 140, 199, 268, 201, 142, 24, 227, 63, 60, 103, 53, 102, 222, 95]
test_index2 = [0, 1, 4, 7, 10, 11, 14, 16, 19, 20, 29, 37, 42, 55, 57, 58, 61, 64, 66, 68, 71, 78, 83, 85, 91, 94, 104, 109, 110, 111, 120, 121, 122, 123, 128, 129, 131, 132, 141, 144, 148, 151, 153, 154, 158, 161, 164, 165, 167, 169, 173, 176, 179, 180, 183, 184, 187, 189, 191, 192, 195, 197, 198, 204, 208, 211, 212, 213, 215, 216, 217, 221, 224, 232, 234, 244, 246, 247, 252, 255, 265, 266]

W = getRandom(m * K) * 100
W=[0.03989223639238065, 0.008953704206280897, 0.01827368485754382, 0.003743320773305305, 0.021220991733608155, 0.011481762230962088, 0.06110101034863459, 0.007348186771532852, 0.03801259018078026, 0.06385616062869234, 0.013608952369887396, 0.0048185631337323565, 0.022081301918202758, 0.12380399758935402, 0.005471077130202131, 0.06171815395489313, 0.02839238922659793, 0.09141488297853134, 0.011363207584894482, 0.04488564539696335, 0.0068898598819220865, 0.0390350077619568, 0.012599804754117186, 0.023033835786903045, 0.008423348834943794, 0.04867251212082652, 0.002263571712174192, 0.034216112255725704, 0.03155607027073213, 0.04288030199477594, 0.006656197627452239, 0.006658927328771733, 0.07870879570167072, 0.006014692511980155, 0.0896971232061955, 0.0027439275248658064, 0.027640239041767916, 0.002012691228107459, 0.011357201485099134, 0.008238531702632963, 0.001957746566003037, 0.001285822506637804, 0.05148782915572098, 0.025628514315699444, 0.012482073490388185, 0.014166597982284214, 0.009447609049641332, 0.0063494918300837574, 0.006888960882772853, 0.010602742989450773, 0.0012300871430582087, 0.07321027683580676, 0.0021731948504467367, 0.025362995481014273, 0.006326152401145779, 0.023722873045623982, 0.01720624073469947, 0.10800193740534281, 0.048862986211967295, 0.00023722892606546972, 0.018738300672233046, 0.0008084079254853257, 0.07668467402548966, 0.002669007440944649, 0.031181691631686773, 0.04370008689587104, 0.0005749153873343752, 0.05064555376055178, 0.0044085066334537595, 0.040273317109283245, 0.05335959165490344, 0.07952092749010883, 0.07487989429411222, 0.0031846612211409776, 0.014719190035027115, 0.00454700793465144, 0.10666117614835179, 0.041699079382269494, 0.041689968383720855, 0.010920878780101511, 0.023461987253858983, 0.0029856009148429763, 0.007435382604461463, 0.05835766983491367, 0.05438850083268515, 0.01975010539686449, 0.00572862558155827, 0.014316392974610993, 0.05522960468004376, 0.00413610616915448, 0.018881948546996017, 0.0055101265851077936, 0.07668036311149273, 0.020315316314383033, 0.011443252281374426, 0.08253031862746704, 0.0563487630915304, 0.011481163853348898, 0.00524479146741641, 0.008004369398799826, 0.042618172146508336, 0.022504302291893458, 0.039859661822434805, 0.0010903777922589533, 0.031047035674303787, 0.0016119031016444086, 0.04001292718602094, 0.0031907488042620096, 0.020587249439405854, 0.0004919780480819178, 0.02345115681152816, 0.003556340519773988, 0.07129168491539012, 0.05169787234238555, 0.05447112739426944, 0.015067932106944352, 0.010429462000443694, 0.012318228467842928, 0.05974155298654041, 0.05638568358300017, 0.00016439053163021288, 0.007901359996916399, 0.08085503750030879, 0.0008688870365260052, 0.024914780595241146, 0.035516095758509295, 0.0012380899619384822, 0.013090984121429559, 0.002663600437620805, 0.005802354628870981, 0.009199619376787298, 0.004255613778405905, 0.056568367654636346, 0.1022767805723518, 0.032507749741792685, 0.008465916753345527, 0.01906417968195402, 0.0167909108518718, 0.02908190947573746, 0.054595107967067735, 0.019339367833553042, 0.0492452840809591, 0.024631063596202502, 0.01917919932380288, 0.047741901821237685, 0.06518096663890524, 0.02467427881354416, 0.056268312759011826, 0.06738277577989546, 0.030086879113268863, 0.06074648806305923, 0.02696990777434002, 0.00027479961084488975, 0.00453086094318195, 0.019062451273303055, 0.0027436042347541052, 0.00018944161658219117, 0.05617458886212191, 0.013132236236332298, 0.007427526522110872, 0.06316769052054019, 0.07815612053449886, 0.0003248125079294729, 0.0632703566133957, 0.00888171650410215, 0.04071756658676358, 0.014085976158413734, 0.019181753027676985, 0.07649666830848617, 0.0020492705430839057, 0.0012270534260736673, 0.06272093627520088, 0.0653243285988416, 0.02847609316741462, 0.04943870480915889, 0.017738884137429564, 0.009517909862214664, 0.009608241944958941, 0.17506402986073924, 0.04328252221360979, 0.004676076838211065, 0.006977611347882343, 0.06375900725885954, 0.0050358383756600785, 0.012859718016057418, 0.007639143284949411, 0.010976391996880257, 0.019908542748205138, 0.01123506128592874, 0.017257307757425972, 0.014255913544956233, 0.030470052281306833, 0.015419845520055066, 0.03251263305000597, 0.0802276549898018, 0.01367933963436543, 0.03241427495124821, 0.03073655778487197, 0.013113802747596813, 0.009508862745198795, 0.021687618439850623, 0.006462414530296853, 0.013443290853382797, 0.014859136551374382, 0.008721648245756997, 0.04068160840075169, 0.01046546768485155, 0.06408126437575451, 0.0037957398534233036, 0.016171214173319223, 0.07677225116324937, 0.029141177916516683, 0.03541316785078763, 0.0039886257586588605, 0.15872423651640347, 0.0058101362201801905, 0.0750410756407917, 0.011356458163313458, 0.027490927585471437, 0.05419733712314542, 0.08121240382179389, 0.0099479589582772, 0.08374574199354043, 0.002010240938693833, 0.028849017357114794, 0.001590605608879174, 0.04268548226537036, 0.01269522391560746, 0.009612816489419217, 0.009342990033580498, 0.019306275775811836, 0.02188275767729448, 0.024949508300418734, 0.03522290893202362, 0.008868467740061989, 0.011565560805322485, 0.035773216343087284, 0.011410305738203674, 0.005655896809402396, 0.015826755774704177, 0.03739777071107732, 0.10570969870220027, 0.039401969391971055, 0.062023582700404874, 0.010085642901263276, 0.013668131648389637, 0.009129379487617363, 0.0015881059081346276, 0.09378134051765175, 0.003052449257580064, 0.08652908442086124, 0.013696248626594167, 0.034200891015552085, 0.17785017547932291, 0.01903513290634218, 0.008411169712902048, 0.010917825025321175, 0.013182403201693397, 0.015368596637991363, 0.007133906778756142, 0.02323110972702944, 0.008042887300407036, 0.03633442165658511, 0.002203276517665208, 0.09024592856216684, 0.010191985834364658, 0.03784582620532619, 0.004303717395731152, 0.06410821213313564, 0.0027258000589400542, 0.006192280972870833, 0.006924192242803256, 0.002984216649624847, 0.00512585618464341, 0.01376951061794593, 0.056536269910111354, 0.07410148419609053, 0.007792624167610048, 0.012744607531522647, 0.021183838959868122, 0.04746106315830077, 0.01842975071301167, 0.004379908451064823, 0.003891956850642781, 0.009143679919638837, 0.11083200391036993, 0.037360424819722036, 0.004732724499998404, 0.04951447827340634, 0.003612969526601872, 0.031902013099223664, 0.011299669643730414, 0.03238576336886744, 0.00941805246083534, 0.004687181199841107, 0.025687909348408, 0.039664159525737375, 0.037262493960935235, 0.07404344729081998, 0.01680217571122685, 0.06307172453067263, 0.06879150550713395, 0.00960015370638095, 0.05437079959608701, 0.022667712516217537, 0.018658422069105528, 0.01660735217487662, 0.019846847104768944, 0.06413113011294745, 0.06002960187278418, 0.09841912723369704, 0.05683579449422514, 0.01788763424485819, 0.027819514371006836, 0.14015559999709828, 0.040356087489126224, 0.014278764438556624, 0.025812365976237754, 0.02303050912098241, 0.05627203335198007, 0.0011847160382939849, 0.02153851674252233, 0.010493253990426022, 0.007605980542015837, 0.015037113819829261, 0.08258595195634136, 0.03161632747333157, 0.08480594905044292, 0.0016382567650173737, 0.04538070039952727, 0.0005427570064097624, 0.043112985606783144]





# 求类1的分组情况
NewArray = np.ones((Class1, T + 1))
# 第0组
W1 = W[0:l1]
for i in range(0, Class1):
    add1 = 0
    for j in range(0, len1):
        add1 += W1[j] * X[i, G1[j]]
    NewArray[i][0] = add1
# 第1组
W2 = W[l1:l1+l2]
for i in range(0, Class1):
    add2 = 0
    for j in range(0, len2):
        add2 += W2[j] * X[i, G2[j]]
    NewArray[i][1] = add2
# 第2组
W3 = W[l2:l3]
for i in range(0, Class1):
    add3 = 0
    for j in range(0, len3):
        add3 += W3[j] * X[i, G3[j]]
    NewArray[i][2] = add3
#第三组
W4 = W[l3:l4]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len4):
        add4 += W4[j] * X[i, G4[j]]
    NewArray[i][3] = add4
#第四组
W5 = W[l4:l5]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len5):
        add4 += W5[j] * X[i, G5[j]]
    NewArray[i][4] = add4
#第五组
W6 = W[l5:l6]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len6):
        add4 += W6[j] * X[i, G6[j]]
    NewArray[i][5] = add4

# 求类2的分组情况
NewArray1 = np.ones((Class2, T + 1)) * 2
W8 = W[l6:l6+l1]
for i in range(Class1, n):
    add1 = 0
    for j in range(0, len1):
        add1 += W8[j] * X[i, G1[j]]
    NewArray1[i - Class1][0] = add1
# 第1组
W9 = W[l6+l1:l6+l2]
for i in range(Class1, n):
    add2 = 0
    for j in range(0, len2):
        add2 += W9[j] * X[i, G2[j]]
    NewArray1[i - Class1][1] = add2
# 第2组
W10 = W[l6+l2:l6+l3]
for i in range(Class1, n):
    add3 = 0
    for j in range(0, len3):
        add3 += W10[j] * X[i, G3[j]]
    NewArray1[i - Class1][2] = add3
#第三组
W11 = W[l6+l3:l6+l4]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len4):
        add4 += W11[j] * X[i, G4[j]]
    NewArray1[i-Class1][3] = add4
#第三组
W12 = W[l6+l4:l6+l5]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len5):
        add4 += W12[j] * X[i, G5[j]]
    NewArray1[i-Class1][4] = add4

#第三组
W13 = W[l6+l5:l6+l6]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len6):
        add4 += W13[j] * X[i, G6[j]]
    NewArray1[i-Class1][5] = add4




# print(NewArray1)
# 合并两个数组，得到真正的合并数据结果
NewArray = np.vstack((NewArray, NewArray1))
print(NewArray)

# 随机抽取样本训练集和测试集样本

X1 = NewArray[0:Class1, :]
X2 = NewArray[Class1:Class1 + Class2, :]

Data1 = X1[train_index1, :]
Data2 = X2[train_index2, :]
trainSet=np.vstack((Data1,Data2))
Y=trainSet[:,T]
trainSet=np.delete(trainSet,T,axis = 1)

testSet1 = np.delete(X1[test_index1, :], T, axis = 1)
testSet2 = np.delete(X2[test_index2, :], T, axis = 1)
trainSet1 = np.delete(Data1, T, axis = 1)
trainSet2 = np.delete(Data2, T, axis = 1)
valSet1=np.delete(X1[val_index1,:],T,axis = 1)
valSet2=np.delete(X2[val_index2,:],T,axis = 1)

# 求各类对应属性的均值和方差
Mean1 = np.mean(trainSet1, axis = 0)
Mean2 = np.mean(trainSet2, axis = 0)
# print(Mean2)
var1 = np.var(trainSet1, axis = 0)
var2 = np.var(trainSet2, axis = 0)



clf=GaussianNB()

clf.fit(trainSet,Y)

C1 = clf.predict(testSet1)
add = sum(C1 == 1)
print("第一类正确数量(总数):", test1)
print(add)
C2 = clf.predict(testSet2)
add1 = sum(C2 == 2)
print("第二类正确数量(总数)：", test2)
print(add1)

print("accuracy:{:.2%}".format((add + add1) / (test1+test2)))



