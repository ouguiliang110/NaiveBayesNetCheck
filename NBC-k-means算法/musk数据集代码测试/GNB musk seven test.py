import numpy as np
import math
from sklearn.naive_bayes import GaussianNB

# 连续型数据分类用正态分布公式
def getPro(theData, mean, var):
    pro = 1 / (math.sqrt(2 * math.pi) * math.sqrt(var)) * math.exp(-(theData - mean) ** 2 / (2 * var))
    return pro


def getRandom(num):
    Ran = np.random.dirichlet(np.ones(num), size = 1)
    Ran = Ran.flatten()
    return Ran


'''
def CountP1(test):
    sum=1
    for i in range(0,60):
       sum*=getPro(test[i],
def CountP2(test):
    sum=1
    for i in  range(0,60):
        sum*=getPro(())
'''

X = np.loadtxt('[018]musk01(0-1).txt')
# 其中有97
m = X.shape[1] - 1  # 属性数量
n = X.shape[0]  # 样本数目
T = 7
K = 2  # 类标记数量
# 主要过程：分组
# 去掉类标记
Class1 = 0
Class2 = 0

for i in X:
    if i[m] == 1:
        Class1 = Class1 + 1
    elif i[m] == 2:
        Class2 = Class2 + 1

train1 = int(Class1 * 0.5)
val1 = int(Class1 * 0.2)
test1 = Class1 - train1 - val1

train2 = int(Class2 * 0.5)
val2 = int(Class2 * 0.2)
test2 = Class2 - train2 - val2

# 随机产生多少个和为1的随机数W
G1 = [16, 23, 63, 89, 98, 124, 158] # 7
G2 = [1, 29, 31, 40, 61, 68, 70, 100, 116, 117, 118, 127, 155]# 3
G3 = [2, 3, 6, 8, 21, 24, 26, 33, 38, 51, 52, 54, 56, 57, 59, 60, 64, 71, 76, 81, 83, 84, 85, 91, 97, 99, 112, 113, 119, 120, 121, 122, 129, 131, 136, 138, 142, 143, 151, 153, 154, 160]
G4 = [13, 14, 15, 41, 45, 58, 114, 123, 135, 147]
G5 = [0, 4, 5, 9, 10, 11, 18, 19, 20, 22, 25, 27, 34, 35, 36, 37, 39, 43, 46, 47, 49, 50, 55, 65, 73, 75, 78, 80, 82, 86, 87, 88, 90, 92, 95, 96, 101, 103, 104, 107, 108, 111, 115, 130, 132, 133, 137, 139, 140, 141, 144, 148, 149, 152, 157, 159, 161, 163]
G6 = [17, 42, 48, 53, 62, 72, 74, 77, 79, 93, 94, 102, 105, 110, 125, 126, 128]
G7 = [7, 12, 28, 30, 32, 44, 66, 67, 69, 106, 109, 134, 145, 146, 150, 156, 162, 164, 165]  # 12
len1=len(G1)
len2=len(G2)
len3=len(G3)
len4=len(G4)
len5=len(G5)
len6=len(G6)
len7=len(G7)
l1=len1
l2=len1+len2
l3=len1+len2+len3
l4=len1+len2+len3+len4
l5=len1+len2+len3+len4+len5
l6=len1+len2+len3+len4+len5+len6
l7=len1+len2+len3+len4+len5+len6+len7
#随机训练集，验证集，测试集区

idx = np.random.choice(np.arange(Class1), size = train1, replace = False)
train_index1 = np.array(idx)
val_index1 = np.random.choice(np.delete(np.arange(Class1), train_index1), size = val1, replace = False)
test_index1 = np.delete(np.arange(Class1), np.append(train_index1, val_index1))

idx1 = np.random.choice(np.arange(Class2), size = train2, replace = False)
train_index2 = np.array(idx1)
val_index2 = np.random.choice(np.delete(np.arange(Class2), train_index2), size = val2, replace = False)
test_index2 = np.delete(np.arange(Class2), np.append(train_index2, val_index2))

print("train_index1 =",list(train_index1))
print("val_index1 =",list(val_index1))
print("test_index1 =",list(test_index1))
print("train_index2 =",list(train_index2))
print("val_index2 =",list(val_index2))
print("test_index2 =",list(test_index2))




#确认训练集，验证集，测试集区
train_index1 = [108, 37, 149, 116, 29, 164, 7, 59, 161, 68, 174, 179, 151, 8, 189, 86, 16, 26, 153, 62, 176, 47, 165, 127, 60, 79, 19, 87, 201, 91, 120, 113, 28, 21, 185, 67, 117, 46, 122, 178, 194, 41, 71, 77, 180, 78, 96, 0, 12, 31, 160, 45, 76, 20, 147, 154, 49, 148, 188, 22, 1, 138, 134, 157, 88, 38, 124, 85, 183, 103, 94, 170, 56, 166, 171, 5, 4, 17, 66, 159, 123, 54, 83, 42, 119, 52, 44, 141, 146, 72, 70, 184, 35, 82, 50, 140, 92, 102, 27, 206, 155, 139, 187]
val_index1 = [158, 190, 135, 131, 169, 43, 110, 128, 39, 13, 63, 196, 129, 111, 3, 118, 89, 10, 98, 53, 58, 51, 202, 65, 55, 168, 100, 173, 18, 84, 33, 177, 115, 172, 106, 30, 167, 114, 133, 2, 61]
test_index1 = [6, 9, 11, 14, 15, 23, 24, 25, 32, 34, 36, 40, 48, 57, 64, 69, 73, 74, 75, 80, 81, 90, 93, 95, 97, 99, 101, 104, 105, 107, 109, 112, 121, 125, 126, 130, 132, 136, 137, 142, 143, 144, 145, 150, 152, 156, 162, 163, 175, 181, 182, 186, 191, 192, 193, 195, 197, 198, 199, 200, 203, 204, 205]
train_index2 = [111, 110, 247, 129, 99, 31, 171, 95, 157, 70, 214, 265, 236, 34, 144, 51, 67, 225, 133, 140, 242, 14, 43, 173, 59, 199, 112, 93, 261, 264, 156, 72, 206, 22, 50, 62, 119, 2, 132, 245, 263, 260, 121, 107, 47, 97, 82, 160, 71, 57, 116, 120, 240, 198, 28, 178, 87, 252, 106, 202, 137, 16, 266, 126, 169, 243, 33, 150, 4, 96, 45, 172, 207, 68, 48, 24, 175, 226, 190, 254, 268, 161, 149, 7, 267, 74, 256, 127, 251, 215, 177, 176, 187, 231, 248, 148, 103, 40, 73, 79, 85, 3, 42, 189, 159, 152, 77, 124, 113, 186, 9, 222, 23, 46, 15, 21, 30, 229, 210, 153, 134, 118, 61, 227, 88, 181, 44, 164, 235, 54, 12, 239, 53, 194]
val_index2 = [123, 195, 130, 211, 63, 78, 29, 18, 196, 80, 5, 204, 250, 212, 238, 101, 8, 151, 170, 223, 6, 228, 92, 36, 162, 257, 221, 83, 75, 128, 142, 255, 197, 146, 183, 185, 141, 213, 167, 26, 218, 191, 249, 49, 32, 64, 38, 37, 232, 168, 258, 94, 76]
test_index2 = [0, 1, 10, 11, 13, 17, 19, 20, 25, 27, 35, 39, 41, 52, 55, 56, 58, 60, 65, 66, 69, 81, 84, 86, 89, 90, 91, 98, 100, 102, 104, 105, 108, 109, 114, 115, 117, 122, 125, 131, 135, 136, 138, 139, 143, 145, 147, 154, 155, 158, 163, 165, 166, 174, 179, 180, 182, 184, 188, 192, 193, 200, 201, 203, 205, 208, 209, 216, 217, 219, 220, 224, 230, 233, 234, 237, 241, 244, 246, 253, 259, 262]


W = getRandom(m * K) * 100
W=[0.005232542611588637, 0.03307724192503764, 0.017683084231226484, 0.0037043307590532686, 0.03462439763318456, 0.056023594751066025, 0.005856823636251761, 0.10119359859999388, 0.015197369268487242, 0.057389752356866, 0.028449196340409214, 0.05531876744079688, 0.02443751688698892, 0.034384390652450726, 0.14365629754896195, 0.002539797419463357, 0.02612066954061933, 0.06137536758660897, 0.010385541155251185, 0.06650363450034064, 0.004046373822763403, 0.02869226251890588, 0.019088822941837096, 0.05444522694840692, 0.033799519104973516, 0.03163793068198484, 0.049552186822890774, 0.019803073212320388, 0.009694398425724282, 0.007258472920483668, 0.013652564885488945, 0.04286582840467916, 0.019532049768103354, 0.05962977275367237, 0.03208140906404353, 0.05174767937106612, 0.00393204823268712, 0.01895399829570913, 0.04234128662549307, 0.013906991081508599, 0.03904012478557418, 0.021636718684120666, 0.02469784474112438, 0.04532565627155824, 0.00897528492336651, 0.011498784215824459, 0.016446861420832744, 0.07783843805973964, 0.05612408288199356, 0.04153803157935991, 0.0731612702909279, 0.012603283818587819, 0.04532700904724202, 0.001930986865711255, 0.016575677280086222, 0.010205454583705819, 0.05412107835389805, 0.04341379200326432, 0.0009595005290689016, 0.012590252739085612, 0.021024839133747846, 0.002445655224978019, 0.003198082291680311, 0.08094015690019216, 0.023202684769175246, 0.00567716988813589, 0.002546121398630395, 0.008261384453628857, 0.022847612904930925, 0.004314933427649009, 0.06266870305924292, 0.005340268959359741, 0.010429141194286666, 0.014604017197339777, 0.037781381565885126, 0.021410325935911162, 0.0005649554290801412, 0.013550840008611953, 0.07338043721717714, 0.06848849190140122, 0.01948903705027137, 0.028270024805889638, 0.02393715816836007, 0.07565432904571448, 0.027499877449666203, 0.008674268069533222, 0.0019259343461455274, 0.04549248501622616, 0.04268012918976609, 0.0037310079258512305, 0.00578505592118117, 0.03498045751381435, 0.08123370896441247, 0.05574630792214757, 0.020382451441878172, 0.09392432431051281, 0.004633039353218418, 0.0868466809268727, 0.0010475511899425327, 0.006599081158751145, 0.008591197045237201, 0.03563488984586308, 0.02622695132626751, 0.06458676816210451, 0.01476770544409439, 0.08537442135313422, 0.014067072147745309, 0.002642040128246961, 0.006916001918147681, 0.010501089598554891, 0.019895137238291158, 0.04230840408752404, 0.012247427555689631, 0.007462613076129148, 0.008391467875205911, 0.03107844929215922, 0.033198570636171786, 0.007093340358282226, 0.04482280565364449, 0.021622125903657842, 0.043534139740933245, 0.0006014997018207298, 0.09660974037378381, 0.11356799486464611, 0.06672949524128355, 0.01588031104620732, 0.09298891505860582, 0.07205461723447158, 0.00855830454896954, 0.012606331155723802, 0.04474220385310197, 0.023879145836300932, 0.06990402385782386, 0.046405137362128285, 0.021574336094881102, 0.01531278071110206, 0.029684686055538423, 0.018482449935519146, 0.018905608818223126, 0.006293228404855317, 0.009316753911346945, 0.030308927316554088, 0.06373375501909974, 0.05273060021126107, 0.02861336458368157, 0.07727329030113783, 0.07200929926368631, 0.020371386782309764, 0.002695778890524096, 0.0016723888709048063, 0.0012340393796898513, 0.10288768734220449, 0.036513780665432156, 0.00391755897081756, 0.028292373333635226, 0.012436244253099265, 0.04249566392629209, 0.021056516239264907, 0.02576058301394639, 0.07794776119882288, 0.011794564802382076, 0.06720199419581954, 0.005121103430655496, 0.08394402571788058, 0.013943294897573656, 0.01757439860228321, 0.002066169630449424, 0.020152795904808514, 0.007909869981098576, 0.09019674607080032, 0.024327040911524562, 0.003939396768754323, 0.032253884313835454, 0.010007150092147622, 0.004318173318311752, 0.02711901336037311, 0.04644657570399841, 0.029485462987590312, 0.02186868764478958, 0.011177436314056478, 0.019898547443300614, 0.004217250118334978, 0.01651256104508739, 0.013092187762400802, 0.08504498658938431, 0.08439231814621853, 0.0052178392853114266, 0.032670969395128105, 0.009205036619614053, 0.0574713044877916, 0.02561515670498425, 0.010346315378791455, 0.02962344412196735, 0.0011201823148153406, 0.0005662985135670194, 0.01433192338644923, 0.011902665503294257, 0.012192069649551884, 0.003530703092985461, 0.03461874467774924, 0.012051319423756297, 0.007156940632834611, 0.008786308171468872, 0.050916873808698714, 0.006902721069012949, 0.0030804835450264924, 0.016989717760663155, 0.048247543802932454, 0.03784184155350408, 0.004325838197415791, 0.02122394240025109, 0.021500379576223118, 0.04869587916957464, 0.042220222269613895, 0.025070950604890837, 0.0063394195237271005, 0.017949808891444652, 0.02311950040508128, 0.04249714211967363, 0.02164471476792806, 0.0653505869838362, 0.017578848522200943, 0.010584526128795492, 0.008985010067545226, 0.009932290114728288, 0.14004146314642132, 0.0209344281052295, 0.05223688389778565, 0.03961003809222341, 0.05852858523774149, 0.008860773863938567, 0.042804979803263525, 0.009317003978289476, 0.06390161780737041, 0.06659288193521527, 0.11432336751329206, 0.1169668823408617, 0.09119565591297536, 0.07973185489690238, 0.06283714363047335, 0.048659765504025336, 0.02465778107573905, 0.01502832272823389, 0.0054517177747630974, 0.09262374848103315, 0.004025413044577063, 0.015082283941651236, 2.8201878360761278e-05, 0.01992860031894306, 0.044496196521999695, 0.014426856296813916, 0.007451808475258513, 0.03683833922805185, 0.01430764830915981, 0.005301135907284323, 0.07901457306815357, 0.02446232613102585, 0.03864025868666475, 0.035298161630918444, 0.006821724784790836, 0.09068342048809694, 0.0023515437145386468, 0.01840303006628111, 0.007364010021010023, 0.006033663222817046, 0.025571194080358503, 0.017687213060654804, 0.01481935810387442, 0.03531952337388901, 0.003939320565657863, 0.017795134596191444, 0.011586961188149536, 0.06247620951973136, 0.030549862463975362, 0.00880260333871506, 0.07845232615648226, 0.007337873276753817, 0.06101657285449314, 0.02518549219820167, 0.034757965220582485, 0.00835343543401049, 0.024818691965080745, 0.028922724549917157, 0.007540274496271524, 0.003164540696472926, 0.0091070456563394, 0.01819346245217593, 0.029668354139684883, 0.02461927119224206, 0.035902409807431195, 0.03848483082914533, 0.08996954651357567, 0.018035209065348952, 0.00883575676038203, 0.015741451191058114, 0.07257420016065579, 0.0025816323634309532, 0.013077840407054491, 0.007372741112487721, 0.01117473342166789, 0.0014807624009316464, 0.05107965744499579, 0.020020442983345394, 0.0019013757837930633, 0.09153378979401164, 0.005838863007368958, 0.002603586014797851, 0.009124180286920247, 0.008713295956682864, 0.08004504694713574, 0.050431420515929144, 0.008180113747121175, 0.002507902235972863, 0.0012969398107362628, 0.01334033245705011, 0.024431563620964675, 0.029121154522471153, 0.030842121743291356, 0.008065773412486217, 0.03666793841328533, 0.027953354997414182, 0.0444508624303041, 0.0033462329164401427, 0.11282336341986804, 0.013301719430219244, 0.004720998318510772, 0.010581406595419451, 0.00022816971586427011, 0.010452450662197929, 0.02667134981346357, 0.012169097099980371, 0.013916542980571838]



# 求类1的分组情况
NewArray = np.ones((Class1, T + 1))
# 第0组
W1 = W[0:l1]
for i in range(0, Class1):
    add1 = 0
    for j in range(0, len1):
        add1 += W1[j] * X[i, G1[j]]
    NewArray[i][0] = add1
# 第1组
W2 = W[l1:l1+l2]
for i in range(0, Class1):
    add2 = 0
    for j in range(0, len2):
        add2 += W2[j] * X[i, G2[j]]
    NewArray[i][1] = add2
# 第2组
W3 = W[l2:l3]
for i in range(0, Class1):
    add3 = 0
    for j in range(0, len3):
        add3 += W3[j] * X[i, G3[j]]
    NewArray[i][2] = add3
#第三组
W4 = W[l3:l4]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len4):
        add4 += W4[j] * X[i, G4[j]]
    NewArray[i][3] = add4
#第四组
W5 = W[l4:l5]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len5):
        add4 += W5[j] * X[i, G5[j]]
    NewArray[i][4] = add4
#第五组
W6 = W[l5:l6]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len6):
        add4 += W6[j] * X[i, G6[j]]
    NewArray[i][5] = add4

#第六组
W7 = W[l6:l7]
for i in range(0, Class1):
    add4 = 0
    for j in range(0, len7):
        add4 += W7[j] * X[i, G7[j]]
    NewArray[i][6] = add4
# print(NewArray)

# 求类2的分组情况
NewArray1 = np.ones((Class2, T + 1)) * 2
W8 = W[l7:l7+l1]
for i in range(Class1, n):
    add1 = 0
    for j in range(0, len1):
        add1 += W8[j] * X[i, G1[j]]
    NewArray1[i - Class1][0] = add1
# 第1组
W9 = W[l7+l1:l7+l2]
for i in range(Class1, n):
    add2 = 0
    for j in range(0, len2):
        add2 += W9[j] * X[i, G2[j]]
    NewArray1[i - Class1][1] = add2
# 第2组
W10 = W[l7+l2:l7+l3]
for i in range(Class1, n):
    add3 = 0
    for j in range(0, len3):
        add3 += W10[j] * X[i, G3[j]]
    NewArray1[i - Class1][2] = add3
#第三组
W11 = W[l7+l3:l7+l4]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len4):
        add4 += W11[j] * X[i, G4[j]]
    NewArray1[i-Class1][3] = add4
#第三组
W12 = W[l7+l4:l7+l5]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len5):
        add4 += W12[j] * X[i, G5[j]]
    NewArray1[i-Class1][4] = add4

#第三组
W13 = W[l7+l5:l7+l6]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len6):
        add4 += W13[j] * X[i, G6[j]]
    NewArray1[i-Class1][5] = add4

#第三组
W14 = W[l7+l6:l7+l7]
for i in range(Class1, n):
    add4 = 0
    for j in range(0, len7):
        add4 += W14[j] * X[i, G7[j]]
    NewArray1[i-Class1][6] = add4



# print(NewArray1)
# 合并两个数组，得到真正的合并数据结果
NewArray = np.vstack((NewArray, NewArray1))
print(NewArray)

# 随机抽取样本训练集和测试集样本

X1 = NewArray[0:Class1, :]
X2 = NewArray[Class1:Class1 + Class2, :]

Data1 = X1[train_index1, :]
Data2 = X2[train_index2, :]
trainSet=np.vstack((Data1,Data2))
Y=trainSet[:,T]
trainSet=np.delete(trainSet,T,axis = 1)

testSet1 = np.delete(X1[test_index1, :], T, axis = 1)
testSet2 = np.delete(X2[test_index2, :], T, axis = 1)
trainSet1 = np.delete(Data1, T, axis = 1)
trainSet2 = np.delete(Data2, T, axis = 1)
valSet1=np.delete(X1[val_index1,:],T,axis = 1)
valSet2=np.delete(X2[val_index2,:],T,axis = 1)

# 求各类对应属性的均值和方差
Mean1 = np.mean(trainSet1, axis = 0)
Mean2 = np.mean(trainSet2, axis = 0)
# print(Mean2)
var1 = np.var(trainSet1, axis = 0)
var2 = np.var(trainSet2, axis = 0)



clf=GaussianNB()

clf.fit(trainSet,Y)

C1 = clf.predict(testSet1)
add = sum(C1 == 1)
print("第一类正确数量(总数):", test1)
print(add)
C2 = clf.predict(testSet2)
add1 = sum(C2 == 2)
print("第二类正确数量(总数)：", test2)
print(add1)

print("accuracy:{:.2%}".format((add + add1) / (test1+test2)))



